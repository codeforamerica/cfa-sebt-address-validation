{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04fef6b0",
   "metadata": {},
   "source": [
    "## Ensure prerequisites are installed\n",
    "1. Install libpostal\n",
    "    - Instructions\n",
    "        - [Mac/Linux instructions](https://github.com/openvenues/libpostal?tab=readme-ov-file#installation-maclinux)\n",
    "        - [Windows instructions](https://github.com/openvenues/libpostal?tab=readme-ov-file#installation-windows)\n",
    "    - In building these examples, we used the optional [Senzing data model](https://github.com/Senzing/libpostal-data) for libpostal. Instructions for using this model are [here](https://github.com/openvenues/libpostal?tab=readme-ov-file#installation-with-an-alternative-data-model).\n",
    "\n",
    "2. Install Python dependencies\n",
    "    ```bash\n",
    "    pip3 install -r requirements.txt\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d5b3c",
   "metadata": {},
   "source": [
    "## Create random sampling of known addresses with various string edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "def random_string_edit(text, probability=1.0):\n",
    "    \"\"\"Applies a random edit to the input string with a given probability.\"\"\"\n",
    "    if random.random() < probability:\n",
    "        operation = random.choice(['insert', 'delete', 'substitute', 'transpose'])\n",
    "        index = random.randrange(len(text) + 1)  # +1 for potential insertion at the end\n",
    "\n",
    "        if operation == 'insert':\n",
    "            random_char = random.choice(string.ascii_uppercase + string.digits + ' ')\n",
    "            return text[:index] + random_char + text[index:]\n",
    "        elif operation == 'delete' and text:  # Ensure text is not empty\n",
    "            return text[:index] + text[index+1:]\n",
    "        elif operation == 'substitute' and text:\n",
    "            random_char = random.choice(string.ascii_uppercase + string.digits + ' ')\n",
    "            return text[:index] + random_char + text[index+1:]\n",
    "        elif operation == 'transpose' and len(text) >= 2:\n",
    "            idx1 = random.randrange(len(text) - 1)\n",
    "            idx2 = idx1 + 1\n",
    "            list_text = list(text)\n",
    "            list_text[idx1], list_text[idx2] = list_text[idx2], list_text[idx1]\n",
    "            return \"\".join(list_text)\n",
    "    return text  # No edit applied\n",
    "\n",
    "def transform_row(row):\n",
    "    # ADDRESS_NUMBER,ADDRESS_NUMBER_SUFFIX,STREET_NAME,STREET_TYPE,QUADRANT\n",
    "    segment_to_edit = f\"{row.STREET_NAME} {row.STREET_TYPE} {row.QUADRANT}\"\n",
    "    edited_segment = random_string_edit(random_string_edit(segment_to_edit))\n",
    "    address_number_suffix = row.ADDRESS_NUMBER_SUFFIX if row.ADDRESS_NUMBER_SUFFIX != pd.NA else \"\"\n",
    "    print(address_number_suffix)\n",
    "    return f\"{row.ADDRESS_NUMBER} {address_number_suffix} {edited_segment}, {row.CITY}, {row.STATE} {row.ZIPCODE}\"\n",
    "    \n",
    "\n",
    "mar_address_points = pd.read_csv('./data/mar_address_points.csv', header=0, low_memory=False, dtype={\n",
    "    'ADDRESS_NUMBER': 'string',\n",
    "    # 'ADDRESS_NUMBER_SUFFIX': 'string',\n",
    "    'ZIPCODE': 'string'\n",
    "})\n",
    "mar_address_points = mar_address_points[mar_address_points['ADDRESS_TYPE'] == 'ADDRESS']\n",
    "\n",
    "sampled = mar_address_points.sample(n=25)\n",
    "transformed = sampled.apply(transform_row, axis=1)\n",
    "\n",
    "df_output = pd.DataFrame({ 'address': transformed, 'original': sampled['ADDRESS'] })\n",
    "df_output.to_csv('./data/address_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f1e5a",
   "metadata": {},
   "source": [
    "## Parsing addresses for pre-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84022959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from postal.parser import parse_address\n",
    "\n",
    "output_directory = './output'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "def process_row(row):\n",
    "    try:\n",
    "        formatted=f\"{row.ADDRESS} {row.CITY} {row.STATE} {row.ZIPCODE}\"\n",
    "        parsed = parse_address(formatted, language='en', country='us')\n",
    "\n",
    "        keys = [item[1] for item in parsed]\n",
    "        values = [item[0] for item in parsed]\n",
    "\n",
    "        ParseResult = namedtuple('ParseResult', keys)\n",
    "        parse_result = ParseResult(*values)\n",
    "\n",
    "        if hasattr(parse_result, 'house_number') and hasattr(parse_result, 'road'):\n",
    "            return parse_result, True\n",
    "        else:\n",
    "            return parse_result, False\n",
    "    except ValueError:\n",
    "        return None, False\n",
    "\n",
    "mar_address_points = pd.read_csv('./data/mar_address_points.csv', header=0, low_memory=False, dtype={\n",
    "    'ADDRESS_NUMBER': 'string',\n",
    "    # 'ADDRESS_NUMBER_SUFFIX': 'string',\n",
    "    'ZIPCODE': 'string'\n",
    "})\n",
    "\n",
    "street_present = []\n",
    "\n",
    "for row in mar_address_points.itertuples():\n",
    "    result = process_row(row)\n",
    "    processed_data, has_street = result\n",
    "    \n",
    "    if has_street == True:\n",
    "        street_present.append(processed_data._asdict())\n",
    "if street_present:\n",
    "    df_street_present = pd.DataFrame(street_present)\n",
    "    df_street_present.to_csv(os.path.join(output_directory, 'street_present.csv'))\n",
    "else:\n",
    "    print('No output to write for street_present')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10255a",
   "metadata": {},
   "source": [
    "## Building an inverted index for near-dupe matching\n",
    "This sample uses the [open-data list of address points](https://opendata.dc.gov/datasets/DCGIS::address-points/about) from the Washington, D.C. [Master Address Repository (MAR)](https://opendata.dc.gov/pages/addressing-in-dc). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MAR index from file...\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from collections import defaultdict\n",
    "# from functools import reduce\n",
    "# # from fuzzywuzzy import fuzz\n",
    "# from postal.parser import parse_address\n",
    "# from postal.expand import expand_address\n",
    "# from postal.near_dupe import near_dupe_hashes\n",
    "\n",
    "# mar_address_points = pd.read_csv('./data/mar_address_points.csv', header=0, low_memory=False, dtype={\n",
    "#     'ADDRESS_NUMBER': 'string',\n",
    "#     'ADDRESS_NUMBER_SUFFIX': 'string',\n",
    "#     'ZIPCODE': 'string'\n",
    "# })\n",
    "# mar_address_points = mar_address_points[mar_address_points['ADDRESS_TYPE'] == 'ADDRESS']\n",
    "\n",
    "# flat_map = lambda f, xs, **kwargs: reduce(lambda a, b: a + b, map(lambda x: f(x, **kwargs), xs))\n",
    "\n",
    "# def get_expansion_hash(expansion, exclude_house_number=False):\n",
    "#     parsed_address = parse_address(expansion, language='en', country='us')\n",
    "#     labels=[]\n",
    "#     values=[]\n",
    "#     for value, label in parsed_address:\n",
    "#         if exclude_house_number == True and label == 'house_number':\n",
    "#             continue\n",
    "#         labels.append(label)\n",
    "#         values.append(value)\n",
    "#     return near_dupe_hashes(labels, values, languages=['en'], address_only_keys=True, )\n",
    "\n",
    "# def get_hashes(address, exclude_house_number=False):\n",
    "#     expansions = expand_address(address, languages=['en'])\n",
    "#     return flat_map(get_expansion_hash, expansions, exclude_house_number=exclude_house_number)\n",
    "\n",
    "# def build_inverted_index() -> defaultdict:\n",
    "#     index = defaultdict(set)\n",
    "\n",
    "#     for row in mar_address_points.itertuples():\n",
    "#         formatted=f\"{row.ADDRESS}, {row.CITY}, {row.STATE} {row.ZIPCODE}\"\n",
    "#         full_hashes = set(get_hashes(formatted))\n",
    "#         no_house_number_hashes = set(get_hashes(formatted, exclude_house_number=True))\n",
    "#         hashes = list(full_hashes | no_house_number_hashes)\n",
    "#         result = (row.MAR_ID, row.ADDRESS, str(row.ADDRESS_NUMBER), row.ADDRESS_NUMBER_SUFFIX, row.STREET_NAME, row.STREET_TYPE, row.QUADRANT, row.CITY, row.STATE, row.ZIPCODE, row.COUNTRY)\n",
    "\n",
    "#         for hash in hashes:\n",
    "#             index[hash].add(result)\n",
    "\n",
    "#     return index\n",
    "\n",
    "# def load_or_build_inverted_index() -> defaultdict:\n",
    "#     try:\n",
    "#         with open('./data/address_index.pkl', 'rb') as read:\n",
    "#             print('Loading MAR index from file...')\n",
    "#             return pickle.load(read)\n",
    "#     except Exception:\n",
    "#         print('Building MAR index from scratch...')\n",
    "#         index = build_inverted_index()\n",
    "#         with open('./data/address_index.pkl', 'wb') as write:\n",
    "#             pickle.dump(index, write)\n",
    "#         return index\n",
    "\n",
    "# # df_samples = pd.read_csv(\"./data/address_samples.csv\")\n",
    "\n",
    "# inverted_index = load_or_build_inverted_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4dc9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe5eb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Blocking time: 0.05 seconds\n",
      "Predict time: 0.77 seconds\n",
      "\n",
      " -- WARNING --\n",
      "You have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary.  To produce predictions the following untrained trained parameters will use default values.\n",
      "Comparison: 'address':\n",
      "    m values not fully trained\n",
      "Comparison: 'address':\n",
      "    u values not fully trained\n",
      "Comparison: 'zip':\n",
      "    m values not fully trained\n",
      "Comparison: 'zip':\n",
      "    u values not fully trained\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        unique_id_l  unique_id_r  match_probability  \\\n",
      "54863         75011            8           0.995255   \n",
      "286137       295422            1           0.995255   \n",
      "\n",
      "                                            address_l  \\\n",
      "54863   1038 BARNABY TERRACE SE, WASHINGTON, DC 20032   \n",
      "286137       4511 3RD STREET SE, WASHINGTON, DC 20032   \n",
      "\n",
      "                                            address_r  \n",
      "54863   1038 BARNABY TERRACE SE, WASHINGTON, DC 20032  \n",
      "286137       4511 3RD STREET SE, WASHINGTON, DC 20032  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from postal.parser import parse_address\n",
    "from splink import DuckDBAPI, Linker, SettingsCreator, block_on\n",
    "import splink.comparison_library as cl\n",
    "\n",
    "def parse_zip_code(address):\n",
    "    parsed = parse_address(address)\n",
    "    return next((n[0] for n in parsed if n[1] == 'postcode'), None)\n",
    "\n",
    "df_mar = pd.read_csv('./data/mar_address_points.csv', header=0, low_memory=False, dtype={\n",
    "    'ADDRESS_NUMBER': 'string',\n",
    "    'ADDRESS_NUMBER_SUFFIX': 'string',\n",
    "    'ZIPCODE': 'string'\n",
    "})\n",
    "df_mar[\"FULL_ADDRESS\"] = df_mar[\"ADDRESS\"].astype(str) + ', ' + df_mar[\"CITY\"].astype(str) + ', ' + df_mar[\"STATE\"].astype(str) + ' ' + df_mar[\"ZIPCODE\"]\n",
    "mar_columns=[\"MAR_ID\", \"FULL_ADDRESS\", \"ZIPCODE\"]\n",
    "mar_new_columns=[\"unique_id\",\"address\",\"zip\"]\n",
    "mar_col_map=dict(zip(mar_columns, mar_new_columns))\n",
    "df_mar = df_mar[mar_columns].rename(columns=mar_col_map)\n",
    "\n",
    "df_samples = pd.read_csv(\"./data/address_samples.csv\")\n",
    "df_samples[\"zip\"] = df_samples[\"address\"].apply(parse_zip_code)\n",
    "df_samples[\"unique_id\"] = df_samples.reset_index(drop=True).index + 1\n",
    "\n",
    "df_mar_duck = duckdb.sql(\"SELECT * FROM df_mar\")\n",
    "df_samples_duck = duckdb.sql(\"SELECT * FROM df_samples\")\n",
    "\n",
    "con = duckdb.connect(\":default:\")\n",
    "db_api = DuckDBAPI(connection=con)\n",
    "\n",
    "settings = SettingsCreator(\n",
    "    link_type=\"link_only\",\n",
    "    blocking_rules_to_generate_predictions=[\n",
    "        block_on(\"zip\")\n",
    "    ],\n",
    "    comparisons=[\n",
    "        cl.LevenshteinAtThresholds(\"address\", [2, 4]),\n",
    "        cl.ExactMatch(\"zip\"),\n",
    "    ],\n",
    "    probability_two_random_records_match=0.0002,\n",
    "    retain_intermediate_calculation_columns=True,\n",
    "    retain_matching_columns=True,\n",
    ")\n",
    "\n",
    "linker = Linker([df_mar_duck, df_samples_duck], settings, db_api)\n",
    "\n",
    "# Execute the record linkage\n",
    "splink_df_predict = linker.inference.predict()\n",
    "\n",
    "df_predict = splink_df_predict.as_pandas_dataframe()\n",
    "threshold = 0.65\n",
    "df_predict = df_predict[df_predict[\"match_probability\"] > threshold]\n",
    "output_cols = [\"unique_id_l\", \"unique_id_r\", \"match_probability\", \"address_l\", \"address_r\"]\n",
    "df_predict = df_predict[output_cols]\n",
    "\n",
    "# Display the results\n",
    "print(df_predict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
